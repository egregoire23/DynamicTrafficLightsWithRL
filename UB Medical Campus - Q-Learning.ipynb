{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "47c53f21-3a5a-44cc-9204-4aed8c24b5c9",
   "metadata": {},
   "source": [
    "# Red Light, Green Light: Dynamic Traffic Lights with RL\n",
    "## By Erin Gregoire & Daniel Viola\n",
    "### Spring 2025"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3db5285e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "from sumo_rl import SumoEnvironment\n",
    "import os\n",
    "import pickle\n",
    "class QAgent():\n",
    "    def __init__(self, learning_rate =.1, gamma= 0.9, epsilon=.1):\n",
    "        self.q_table ={}\n",
    "        self.learning_rate=learning_rate\n",
    "        self.gamma= gamma\n",
    "        self.epsilon =epsilon\n",
    "\n",
    "    def chose_action(self, state, light_actions):\n",
    "        if random.random()<=self.epsilon:\n",
    "            return random.choice(light_actions)\n",
    "        else:\n",
    "            q_value = [self.q_table.get((state,ready_set_action),0.0) for ready_set_action in light_actions]\n",
    "            max_q =max(q_value)\n",
    "            Hmm_which_action= [action for action, value in zip(light_actions, q_value) if value ==max_q]\n",
    "            return random.choice(Hmm_which_action)\n",
    "        \n",
    "    def update_q_time(self, state, action, reward, next_state, light_actions):\n",
    "        current_q = self.q_table.get((state, action),0.0)\n",
    "        next_states_q = [self.q_table.get((state,some_actions_homie),0.0) for some_actions_homie in light_actions]\n",
    "        big_man_q =max(next_states_q)\n",
    "        final_update =current_q +self.learning_rate*(reward +self.gamma*(big_man_q) - current_q)\n",
    "        self.q_table[(state,action)]=final_update\n",
    "\n",
    "    def winner_winner_chicken_dinner(self, obs):\n",
    "        phase = int(obs[0])\n",
    "        densities = obs[1:]\n",
    "        \n",
    "        storage_list = []\n",
    "        for d in densities:\n",
    "            storage_list.append(int(min(d * 10, 9)))\n",
    "        \n",
    "        return (phase, *storage_list)\n",
    "    \n",
    "    def save_q_table(self, filename=\"q_table.pkl\"):\n",
    "        with open(filename, \"wb\") as f:\n",
    "            pickle.dump(self.q_table, f)\n",
    "\n",
    "    def load_q_table(self, filename):\n",
    "        with open(filename, \"rb\") as f:\n",
    "            self.q_table = pickle.load(f)\n",
    "    \n",
    "\n",
    "        \n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3aec1fd3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pickle\n",
    "import random\n",
    "from sumo_rl import SumoEnvironment\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#also make sure you have sumo rl installed its not super easy but is necesary to run this\n",
    "\n",
    "\n",
    "\n",
    "import os, pickle, random, numpy as np\n",
    "from sumo_rl import SumoEnvironment\n",
    "\n",
    "os.environ[\"SUMO_HOME\"] = \"C:/Users/alien/AppData/Local/Programs/Python/Python39/Lib/site-packages/sumo\"\n",
    "\n",
    "env = SumoEnvironment(\n",
    "    net_file=\"buffalo/buffalo_final.net.xml\",\n",
    "    route_file=\"buffalo/phase2.rou.xml\",\n",
    "    use_gui=True,\n",
    "    num_seconds=2000,\n",
    "    single_agent=False,\n",
    "    delta_time=5,\n",
    "    reward_fn=\"pressure\",\n",
    ")\n",
    "\n",
    "traffic_ids = list(env.traffic_signals.keys())\n",
    "evaly = True\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "def winner_winner_chicken_dinner(obs, num_green_phases ):\n",
    "    phase_one_hot =obs[:num_green_phases ]\n",
    "    #print(phase_one_hot)\n",
    "\n",
    "    phase_id=int(np.argmax(phase_one_hot))\n",
    "    start=num_green_phases + 1\n",
    "    n_lanes=(len(obs ) -start)//2\n",
    "    #print(n_lanes)\n",
    "    densities =obs[start : start + n_lanes]\n",
    "    densityyy =float(np.max(densities)) if n_lanes else 0.0\n",
    "    if densityyy <0.33:\n",
    "        my_bucket =0\n",
    "        #print(densityyy)\n",
    "    elif densityyy < 0.66:\n",
    "        my_bucket=1\n",
    "\n",
    "    else:\n",
    "        my_bucket= 2\n",
    "\n",
    "    return (phase_id,my_bucket)\n",
    "\n",
    "\n",
    "shared_agent = QAgent(learning_rate=0.1, gamma=0.9, epsilon=0.15)\n",
    "\n",
    "\n",
    "with open(\"q_table_shared_UB.pkl\", \"rb\") as f:\n",
    "    shared_agent.q_table = pickle.load(f)\n",
    "\n",
    "phase_locker ={tid: 0 for tid in traffic_ids}\n",
    "prev_actions = {tid: 0 for tid in traffic_ids}\n",
    "epsilon_decay_every_steps= 400\n",
    "timestep_counter =0\n",
    "num_episodes = 10\n",
    "average_wait_per_car=[]\n",
    "#evaly true is for training, false is for evaluarion \n",
    "if evaly:\n",
    "    for ep in range(num_episodes):\n",
    "        shared_agent.epsilon = 0.0\n",
    "\n",
    "        #print(f\"Episode 440 why wont it count correctly{ep}\")\n",
    "        obs = env.reset()\n",
    "        done = False\n",
    "        total_reward = {tid: 0 for tid in traffic_ids}\n",
    "        while not done:\n",
    "            timestep_counter += 1\n",
    "            #these are commented for evaluation\n",
    "            #if timestep_counter % epsilon_decay_every_steps == 0:\n",
    "                #shared_agent.epsilon = max(0.05, shared_agent.epsilon * 0.995)\n",
    "            actions = {}\n",
    "            for tid in traffic_ids:\n",
    "                le_number_of_green_phases =env.traffic_signals[tid].num_green_phases\n",
    "                state = winner_winner_chicken_dinner(obs[tid], le_number_of_green_phases)\n",
    "                valid_actions = list(range(le_number_of_green_phases))\n",
    "                if phase_locker[tid] <= 0:\n",
    "                    action = shared_agent.chose_action(state,valid_actions)\n",
    "                    prev_actions[tid] = action\n",
    "                    phase_locker[tid] = 3\n",
    "                else:\n",
    "                    action = prev_actions[tid]\n",
    "                    phase_locker[tid] -= 1\n",
    "                actions[tid] = action\n",
    "            next_obs, rewards, dones, info = env.step(actions)\n",
    "            for tid in traffic_ids:\n",
    "                le_number_of_green_phases = env.traffic_signals[tid].num_green_phases\n",
    "                curr_state =winner_winner_chicken_dinner(obs[tid],  le_number_of_green_phases)\n",
    "                next_state= winner_winner_chicken_dinner(next_obs[tid], le_number_of_green_phases)\n",
    "                total_reward[tid] += rewards[tid]\n",
    "            obs = next_obs\n",
    "            done = dones.get(\"__all__\",False)\n",
    "        avg_r= sum(total_reward.values()) / len(traffic_ids)\n",
    "        avg_wait =info[\"system_total_waiting_time\"] / max(info[\"system_total_departed\"], 1)\n",
    "        print(avg_wait)\n",
    "        average_wait_per_car.append(avg_wait)\n",
    "\n",
    "else:\n",
    "    #do not use, instead uncomment epsilon above because this was a whole mess\n",
    "    shared_agent.epsilon = 0.0\n",
    "    eval_episodes = 3\n",
    "    for ev in range(eval_episodes):\n",
    "        obs = env.reset()\n",
    "        done = False\n",
    "        total_eval_reward = {tid: 0 for tid in traffic_ids}\n",
    "        while not done:\n",
    "            actions = {}\n",
    "            for tid in traffic_ids: \n",
    "                le_number_of_green_phases =env.traffic_signals[tid].num_green_phases\n",
    "                state = winner_winner_chicken_dinner(obs[tid], le_number_of_green_phases )  \n",
    "                actions[tid] = shared_agent.chose_action(state, list(range(le_number_of_green_phases)))\n",
    "            next_obs, rewards, dones,info= env.step(actions)\n",
    "            for tid in traffic_ids:\n",
    "                total_eval_reward[tid] +=rewards[tid]\n",
    "            obs =next_obs\n",
    "            done =dones.get(\"__all__\", False)\n",
    "        avg_eval_r = sum(total_eval_reward.values()) / len(traffic_ids)\n",
    "        print(avg_eval_r) \n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "plt.figure(figsize=(10, 5))\n",
    "plt.plot(average_wait_per_car, label=\"Average Waiting Time per Episode\")\n",
    "plt.xlabel(\"Episode\")\n",
    "plt.ylabel(\"Average Wait Time\")\n",
    "plt.title(\"Average Waiting Time\")\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.show()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
