{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "15318e2c-4d93-4bb4-853b-6cefda6a400c",
   "metadata": {},
   "source": [
    "# Red Light, Green Light: Dynamic Traffic Lights with RL\n",
    "## By Erin Gregoire & Daniel Viola\n",
    "### Spring 2025"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3cdd00d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import os\n",
    "import pickle\n",
    "class QAgent():\n",
    "    def __init__(self, learning_rate =.1, gamma= 0.9, epsilon=.1):\n",
    "        self.q_table ={}\n",
    "        self.learning_rate=learning_rate\n",
    "        self.gamma= gamma\n",
    "        self.epsilon =epsilon\n",
    "\n",
    "    def chose_action(self, state, light_actions):\n",
    "        if random.random()<=self.epsilon:\n",
    "            return random.choice(light_actions)\n",
    "        else:\n",
    "            q_value = [self.q_table.get((state,ready_set_action),0.0) for ready_set_action in light_actions]\n",
    "            max_q =max(q_value)\n",
    "            Hmm_which_action= [action for action, value in zip(light_actions, q_value) if value ==max_q]\n",
    "            return random.choice(Hmm_which_action)\n",
    "        \n",
    "    def update_q_time(self, state, action, reward, next_state, light_actions):\n",
    "        current_q = self.q_table.get((state, action),0.0)\n",
    "        next_states_q = [self.q_table.get((state,some_actions_homie),0.0) for some_actions_homie in light_actions]\n",
    "        big_man_q =max(next_states_q)\n",
    "        final_update =current_q +self.learning_rate*(reward +self.gamma*(big_man_q) - current_q)\n",
    "        self.q_table[(state,action)]=final_update\n",
    "\n",
    "    def winner_winner_chicken_dinner(self, obs):\n",
    "        phase = int(obs[0])\n",
    "        densities = obs[1:]\n",
    "        \n",
    "        storage_list = []\n",
    "        for d in densities:\n",
    "            storage_list.append(int(min(d * 10, 9)))\n",
    "        \n",
    "        return (phase, *storage_list)\n",
    "    \n",
    "    def save_q_table(self, filename=\"q_table.pkl\"):\n",
    "        with open(filename, \"wb\") as f:\n",
    "            pickle.dump(self.q_table, f)\n",
    "\n",
    "    def load_q_table(self, filename):\n",
    "        with open(filename, \"rb\") as f:\n",
    "            self.q_table = pickle.load(f)\n",
    "    \n",
    "\n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0bf10ee2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sumo_rl import SumoEnvironment\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "#running the Q agent, it loads the q table because we had alreay trained it at ths point\n",
    "def looping_time():\n",
    "        #you will get an import error with sumo rl if you dont change this to your workspace where you have sumo to whomever may see this\n",
    "\n",
    "    os.environ[\"SUMO_HOME\"] = \"C:/Users/alien/AppData/Local/Programs/Python/Python39/Lib/site-packages/sumo\"\n",
    "\n",
    "    env = SumoEnvironment(\n",
    "        net_file=\"sumo_rl/nets/single-intersection/single-intersection.net.xml\",\n",
    "        route_file=\"sumo_rl/nets/single-intersection/single-intersection.rou.xml\",\n",
    "        use_gui=True,\n",
    "        num_seconds=40000,\n",
    "        single_agent=True,\n",
    "    )\n",
    "\n",
    "    evaluation=True\n",
    "    agent=QAgent()\n",
    "    agent.load_q_table(\"ergregoi_dviola_single_intersection_q_learning_q_table.pkl\")\n",
    "    episodes=20\n",
    "    training_timesteps_per_episode=40000\n",
    "    evaluation_per_episode=40000\n",
    "    average_wait_per_car=[]\n",
    "    total_reward=[]\n",
    "    total_reward_electric_boogaloo=[]\n",
    "    epsilon_storage=.1\n",
    "    decay_rate=.995\n",
    "    for i in range(episodes):\n",
    "        obs_array, info = env.reset()\n",
    "        terminated, truncated=False,False\n",
    "        info={}\n",
    "        reward=0\n",
    "        reward_storage_per_episode_training=0\n",
    "        reward_storage_per_episode_evluation=0\n",
    "        agent.epsilon=epsilon_storage\n",
    "        j=0\n",
    "        previous_wait_time=0\n",
    "        current_wait=0\n",
    "        if evaluation == False:\n",
    "            while not (terminated or truncated):\n",
    "                state=agent.winner_winner_chicken_dinner(obs_array)\n",
    "                action =agent.chose_action(state,list(range(env.action_space.n)))\n",
    "                next_obs_array,reward,terminated,truncated,info =env.step(action)\n",
    "                next_state=agent.winner_winner_chicken_dinner(next_obs_array)\n",
    "\n",
    "                reward_storage_per_episode_training+=reward\n",
    "                agent.update_q_time(state, action, reward, next_state, list(range(env.action_space.n)))\n",
    "                obs_array=next_obs_array\n",
    "                if j%2000== 0:\n",
    "                    epsilon_storage=epsilon_storage*decay_rate\n",
    "                    agent.epsilon=max(epsilon_storage,.1)\n",
    "                j+=1\n",
    "           \n",
    "            total_reward.append(reward_storage_per_episode_training)\n",
    "            agent.save_q_table(\"q_table_episode30.pkl\") #old name, updated to match the submission way that we write it\n",
    "         \n",
    "\n",
    "            reward_storage_per_episode_training=0\n",
    "\n",
    "        obs_array, info = env.reset()\n",
    "        terminated, truncated=False, False\n",
    "        if evaluation ==True:\n",
    "            while not (terminated or truncated):\n",
    "                agent.epsilon=0\n",
    "                state=agent.winner_winner_chicken_dinner(obs_array)\n",
    "                action =agent.chose_action(state,list(range(env.action_space.n)))\n",
    "                next_obs_array,reward,terminated,truncated,info =env.step(action)\n",
    "                    \n",
    "                next_state=agent.winner_winner_chicken_dinner(next_obs_array)\n",
    "                reward_storage_per_episode_evluation+=reward\n",
    "                    \n",
    "                obs_array=next_obs_array\n",
    "            average_wait_per_car.append(info[\"system_total_waiting_time\"]/ max( info[\"system_total_departed\"], 1))\n",
    "            total_reward_electric_boogaloo.append(reward_storage_per_episode_evluation)\n",
    "            reward_storage_per_episode_evluation=0\n",
    "        \n",
    "    env.close()\n",
    " \n",
    "\n",
    "\n",
    "    plt.figure(figsize=(10, 5))\n",
    "    plt.plot(total_reward_electric_boogaloo, label=\"evaluation Reward\")\n",
    "    plt.xlabel(\"Episode\")\n",
    "    plt.ylabel(\"Total Reward\")\n",
    "    plt.title(\"evaluation Reward\")\n",
    "    plt.legend()\n",
    "    plt.grid(True)\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "    plt.figure(figsize=(10, 5))\n",
    "    plt.plot(average_wait_per_car, label=\"average waiting\")\n",
    "    plt.xlabel(\"Episode\")\n",
    "    plt.ylabel(\"wait time\")\n",
    "    plt.title(\"average waiting\")\n",
    "    plt.legend()\n",
    "    plt.grid(True)\n",
    "    plt.show()\n",
    "looping_time()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
