{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "61e2c426-4384-40e3-88c8-5f84b29801be",
   "metadata": {},
   "source": [
    "# Red Light, Green Light: Dynamic Traffic Lights with RL\n",
    "## By Erin Gregoire & Daniel Viola\n",
    "### Spring 2025"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e47124ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import random\n",
    "import copy\n",
    "\n",
    "import torch\n",
    "import torch.optim as optim\n",
    "from torch import nn\n",
    "import numpy as np\n",
    "\n",
    "#to whomever may test this, make sure you have sumo, it is not trivial to setup\n",
    "from sumo_rl import SumoEnvironment\n",
    "\n",
    "\n",
    "class DQNAgent(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        action_space,\n",
    "        initial_epsilon=1.0,    \n",
    "        min_epsilon=0.05,     \n",
    "        epsilon_decay=0.995,   \n",
    "        lr=0.00005,           \n",
    "        gamma=0.98              \n",
    "    ):\n",
    "        super().__init__()\n",
    "\n",
    "\n",
    "        self.layers = nn.Sequential(\n",
    "            nn.Linear(in_features=2, out_features=32),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(in_features=32, out_features=32),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(in_features=32, out_features=action_space),\n",
    "        )\n",
    "\n",
    "        self.target_nn = copy.deepcopy(self.layers)\n",
    "\n",
    "        self.epsilon = initial_epsilon\n",
    "        self.min_epsilon = min_epsilon\n",
    "        self.epsilon_decay = epsilon_decay\n",
    "        self.gamma = gamma\n",
    "\n",
    "        self.loss_fn = nn.MSELoss()\n",
    "        self.optimizer = optim.Adam(self.layers.parameters(), lr=lr)\n",
    "\n",
    "        self.device = torch.device(\"cuda\")\n",
    "        #print(self.device)\n",
    "        self.to(self.device)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.layers(x)\n",
    "\n",
    "    def training_step(self, batch):\n",
    "        obs, action, reward, next_obs, terminated = batch\n",
    "\n",
    "        obs =torch.tensor(obs,dtype=torch.float32, device=self.device)\n",
    "        action= torch.tensor(action, dtype=torch.long,device=self.device)\n",
    "        reward= torch.tensor(reward,dtype=torch.float32, device=self.device)\n",
    "        next_obs= torch.tensor(next_obs,dtype=torch.float32, device=self.device)\n",
    "        terminated= torch.tensor(terminated,dtype=torch.float32, device=self.device)\n",
    "\n",
    "        q_values= self.layers(obs) \n",
    "        q_pred =q_values.gather(1, action.unsqueeze(1)).squeeze(1)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "        with torch.no_grad():\n",
    "\n",
    "            next_q_current=self.layers(next_obs)\n",
    "            best_for_current = next_q_current.argmax(dim =1)\n",
    "\n",
    "            target_next_q =self.target_nn(next_obs) \n",
    "            double_q_time =target_next_q.gather(1, best_for_current.unsqueeze(1)).squeeze(1) \n",
    "\n",
    "            #cha cha real smooth\n",
    "            q_target =reward +(1-terminated)*self.gamma *double_q_time\n",
    "\n",
    "        loss =self.loss_fn(q_pred,q_target)\n",
    "        self.optimizer.zero_grad() \n",
    "        loss.backward()  \n",
    "        self.optimizer.step() \n",
    "\n",
    "    def update_target_network(self):\n",
    "        self.target_nn.load_state_dict( self.layers.state_dict())\n",
    "\n",
    "    def step(self, obs_vector): \n",
    "\n",
    "        if random.random() <self.epsilon: \n",
    "            num_actions =self.layers[-1].out_features \n",
    "            return random.randint(0, num_actions - 1)\n",
    "        else:\n",
    "            obs_tensor = torch.tensor(obs_vector, dtype=torch.float32, device=self.device).unsqueeze(0)\n",
    "            q_values= self.layers(obs_tensor) \n",
    "            action= torch.argmax(q_values, dim=1).item()\n",
    "            \n",
    "            return action\n",
    "        \n",
    "    def winner_winner_chicken_dinner(self, obs):\n",
    "        phase = int(obs[0])\n",
    "        queues = obs[1:] \n",
    "\n",
    "        binned = []\n",
    "        for q in queues:\n",
    "            if q < 0.33:\n",
    "                binned.append(0) \n",
    "            elif q < 0.66:\n",
    "                binned.append(1)\n",
    "            else:\n",
    "                binned.append(2)\n",
    "\n",
    "        return (phase, *binned)\n",
    "\n",
    "\n",
    "class ReplayBuffer:\n",
    "    def __init__(self, max_size):\n",
    "        self.max_size = max_size\n",
    "        self.buffer = [] \n",
    "        self.position = 0\n",
    "\n",
    "    def update_buffer(self, obs, action, reward, next_obs, terminated):\n",
    "        transition = (obs, action, reward, next_obs, terminated)\n",
    "        if len(self.buffer) < self.max_size:\n",
    "\n",
    "            self.buffer.append( transition)  \n",
    "        else:\n",
    "            self.buffer[self.position]=transition\n",
    "        self.position =(self.position+ 1) %self.max_size\n",
    "\n",
    "\n",
    "    def sample_batch(self, batch_size):\n",
    "        \n",
    "        x =random.sample(self.buffer, batch_size)\n",
    "        obs_list =[]\n",
    "        action_list=[]\n",
    "        reward_list =[]\n",
    "        next_obs_list=[]\n",
    "        terminated_list =[]\n",
    "\n",
    "        for transition in x:\n",
    "            obs, action, reward, next_obs, terminated = transition\n",
    "            obs_list.append(obs)\n",
    "            action_list.append(action)\n",
    "            reward_list.append(reward)\n",
    "            next_obs_list.append(next_obs)\n",
    "            terminated_list.append(terminated)\n",
    "\n",
    "\n",
    "        return obs_list, action_list, reward_list, next_obs_list, terminated_list\n",
    "\n",
    "\n",
    "\n",
    "traffic_ids = [\n",
    "    \"901708607\", \"901708581\", \"111358909\",\n",
    "    \"cluster_11117046858_11117046859_111302133_6338690578_#1more\",\n",
    "    \"cluster_11114466314_957732515\",\n",
    "    \"cluster_11114466312_11114466316_111610488\",\n",
    "    \"cluster_111338987_11959023487\",\n",
    "    \"cluster_11117046870_11117046873_11117046875_957663394\",\n",
    "    \"cluster_111569038_6338690548\",\n",
    "    \"265842203\",\n",
    "    \"cluster_11109129474_11117046864_11117046866_111408627_#1more\",\n",
    "\n",
    "\n",
    "\n",
    "    \"cluster_12053387083_12053387276_12053387277_12067122981_#1more\",\n",
    "    \"cluster_111452050_12053387076_12053387077_12053387078_#3more\",\n",
    "    \"cluster_111621643_12054127196_12054127197_12054127198_#1more\",\n",
    "    \"cluster_10906103986_11057916238_11113402178_11113402181_#1more\",\n",
    "    \"cluster_11057916237_11109129500_11109129504_11117046877_#1more\",\n",
    "    \"cluster_111610470_6338703466_6338703467_6338703468_#1more\",\n",
    "    \"cluster_111454858_5844252914\",\n",
    "    \"111571013\",\n",
    "    \"cluster_111403667_1224821721_1224821747_2079395974_#5more\",\n",
    "    \"cluster_111403670_5844252859_5844252860_5844252861_#1more\",\n",
    "    \"111403672\",\n",
    "    \"cluster_111403677_5844252867_5844252868_5844252869_#1more\",\n",
    "    \"256402653\",\n",
    "    \"111480199\",\n",
    "    \"111418028\"\n",
    "\n",
    "]\n",
    "\n",
    "neighbor_map = {\n",
    "    \"111418028\":['PAD', 'PAD', 'PAD', \"PAD\", \"PAD\"],\n",
    "\n",
    "    \"901708607\": [\n",
    "        \"901708581\",\n",
    "        \"cluster_111338987_11959023487\",\n",
    "        \"255961265\",\n",
    "        \"PAD\",\n",
    "        \"PAD\"\n",
    "    ],\n",
    "    \"901708581\": [\n",
    "        \"901708607\",\n",
    "        \"111358909\",\n",
    "        \"PAD\",\n",
    "        \"PAD\",\n",
    "        \"PAD\"\n",
    "    ],\n",
    "    \"111358909\": [\n",
    "        \"901708581\",\n",
    "        \"cluster_11114466314_957732515\",\n",
    "        \"cluster_11117046858_11117046859_111302133_6338690578_#1more\",\n",
    "        \"111393479\",\n",
    "        \"255961265\"\n",
    "    ],\n",
    "    \"cluster_11117046858_11117046859_111302133_6338690578_#1more\": [\n",
    "        \"111358909\",\n",
    "        \"cluster_11114466312_11114466316_111610488\",\n",
    "        \"4450761915\",\n",
    "        \"111349628\",\n",
    "        \"111389483\"\n",
    "    ],\n",
    "    \"cluster_11114466314_957732515\": [\n",
    "        \"cluster_11117046870_11117046873_11117046875_957663394\",\n",
    "        \"cluster_11114466312_11114466316_111610488\",\n",
    "        \"111358909\",\n",
    "        \"111619226\",\n",
    "        \"957732519\"\n",
    "    ],\n",
    "    \"cluster_11114466312_11114466316_111610488\": [\n",
    "        \"cluster_11117046858_11117046859_111302133_6338690578_#1more\",\n",
    "        \"cluster_11114466314_957732515\",\n",
    "        \"cluster_111569038_6338690548\",\n",
    "        \"PAD\",\n",
    "        \"PAD\"\n",
    "    ],\n",
    "    \"cluster_111338987_11959023487\": [\n",
    "        \"901708607\",\n",
    "        \"cluster_11117046870_11117046873_11117046875_957663394\",\n",
    "        \"cluster_12053387083_12053387276_12053387277_12067122981_#1more\",\n",
    "        \"PAD\",\n",
    "        \"PAD\"\n",
    "    ],\n",
    "    \"cluster_11117046870_11117046873_11117046875_957663394\": [\n",
    "        \"cluster_111338987_11959023487\",\n",
    "        \"cluster_11114466314_957732515\",\n",
    "        \"265842203\",\n",
    "        \"3050626911\",\n",
    "        \"957732513\"\n",
    "    ],\n",
    "    \"cluster_111569038_6338690548\": [\n",
    "        \"cluster_11114466312_11114466316_111610488\",\n",
    "        \"cluster_11109129474_11117046864_11117046866_111408627_#1more\",\n",
    "        \"111569042\",\n",
    "        \"PAD\",\n",
    "        \"PAD\"\n",
    "    ],\n",
    "    \"265842203\": [\n",
    "        \"cluster_111452050_12053387076_12053387077_12053387078_#3more\",\n",
    "        \"cluster_12053387083_12053387276_12053387277_12067122981_#1more\",\n",
    "        \"cluster_111621643_12054127196_12054127197_12054127198_#1more\",\n",
    "        \"PAD\",\n",
    "        \"PAD\"\n",
    "    ],\n",
    "    \"cluster_11109129474_11117046864_11117046866_111408627_#1more\": [\n",
    "        \"cluster_111569038_6338690548\",\n",
    "        \"cluster_111610470_6338703466_6338703467_6338703468_#1more\",\n",
    "        \"111408609\",\n",
    "        \"PAD\",\n",
    "        \"PAD\"\n",
    "    ],\n",
    "    \"cluster_12053387083_12053387276_12053387277_12067122981_#1more\": [\n",
    "        \"cluster_111454858_5844252914\",\n",
    "        \"cluster_111452050_12053387076_12053387077_12053387078_#3more\",\n",
    "        \"PAD\",\n",
    "        \"PAD\",\n",
    "        \"PAD\"\n",
    "    ],\n",
    "    \"cluster_111452050_12053387076_12053387077_12053387078_#3more\": [\n",
    "        \"cluster_12053387083_12053387276_12053387277_12067122981_#1more\",\n",
    "        \"265842203\",\n",
    "        \"cluster_111621643_12054127196_12054127197_12054127198_#1more\",\n",
    "        \"cluster_111403667_1224821721_1224821747_2079395974_#5more\",\n",
    "        \"111571013\"\n",
    "    ],\n",
    "    \"cluster_111621643_12054127196_12054127197_12054127198_#1more\": [\n",
    "        \"cluster_111452050_12053387076_12053387077_12053387078_#3more\",\n",
    "        \"cluster_10906103986_11057916238_11113402178_11113402181_#1more\",\n",
    "        \"265842203\",\n",
    "        \"cluster_111403670_5844252859_5844252860_5844252861_#1more\",\n",
    "        \"PAD\"\n",
    "    ],\n",
    "    \"cluster_10906103986_11057916238_11113402178_11113402181_#1more\": [\n",
    "        \"cluster_111621643_12054127196_12054127197_12054127198_#1more\",\n",
    "        \"cluster_11057916237_11109129500_11109129504_11117046877_#1more\",\n",
    "        \"111403672\",\n",
    "        \"PAD\",\n",
    "        \"PAD\"\n",
    "    ],\n",
    "    \"cluster_11057916237_11109129500_11109129504_11117046877_#1more\": [\n",
    "        \"cluster_10906103986_11057916238_11113402178_11113402181_#1more\",\n",
    "        \"cluster_111610470_6338703466_6338703467_6338703468_#1more\",\n",
    "        \"cluster_111403677_5844252867_5844252868_5844252869_#1more\",\n",
    "        \"356366407\",\n",
    "        \"111434392\"\n",
    "    ],\n",
    "    \"cluster_111610470_6338703466_6338703467_6338703468_#1more\": [\n",
    "        \"cluster_11057916237_11109129500_11109129504_11117046877_#1more\",\n",
    "        \"111480199\",\n",
    "        \"cluster_11109129474_11117046864_11117046866_111408627_#1more\",\n",
    "        \"111434392\",\n",
    "        \"356366407\"\n",
    "    ],\n",
    "    \"cluster_111454858_5844252914\": [\n",
    "        \"111571013\",\n",
    "        \"cluster_12053387083_12053387276_12053387277_12067122981_#1more\",\n",
    "        \"PAD\",\n",
    "        \"PAD\",\n",
    "        \"PAD\"\n",
    "    ],\n",
    "    \"111571013\": [\n",
    "        \"cluster_111454858_5844252914\",\n",
    "        \"cluster_111403667_1224821721_1224821747_2079395974_#5more\",\n",
    "        \"cluster_111452050_12053387076_12053387077_12053387078_#3more\",\n",
    "        \"PAD\",\n",
    "        \"PAD\"\n",
    "    ],\n",
    "    \"cluster_111403667_1224821721_1224821747_2079395974_#5more\": [\n",
    "        \"111571013\",\n",
    "        \"cluster_111452050_12053387076_12053387077_12053387078_#3more\",\n",
    "        \"cluster_111403670_5844252859_5844252860_5844252861_#1more\",\n",
    "        \"PAD\",\n",
    "        \"PAD\"\n",
    "    ],\n",
    "    \"cluster_111403670_5844252859_5844252860_5844252861_#1more\": [\n",
    "        \"cluster_111403667_1224821721_1224821747_2079395974_#5more\",\n",
    "        \"cluster_111621643_12054127196_12054127197_12054127198_#1more\",\n",
    "        \"111403672\",\n",
    "        \"PAD\",\n",
    "        \"PAD\"\n",
    "    ],\n",
    "    \"111403672\": [\n",
    "        \"cluster_111403670_5844252859_5844252860_5844252861_#1more\",\n",
    "        \"cluster_10906103986_11057916238_11113402178_11113402181_#1more\",\n",
    "        \"cluster_111403677_5844252867_5844252868_5844252869_#1more\",\n",
    "        \"PAD\",\n",
    "        \"PAD\"\n",
    "    ],\n",
    "    \"cluster_111403677_5844252867_5844252868_5844252869_#1more\": [\n",
    "        \"111403672\",\n",
    "        \"cluster_11057916237_11109129500_11109129504_11117046877_#1more\",\n",
    "        \"256402653\",\n",
    "        \"PAD\",\n",
    "        \"PAD\"\n",
    "    ],\n",
    "    \"256402653\": [\n",
    "        \"cluster_111403677_5844252867_5844252868_5844252869_#1more\",\n",
    "        \"cluster_111610470_6338703466_6338703467_6338703468_#1more\",\n",
    "        \"PAD\",\n",
    "        \"PAD\",\n",
    "        \"PAD\"\n",
    "    ],\n",
    "    \"111480199\": [\n",
    "        \"cluster_111610470_6338703466_6338703467_6338703468_#1more\",\n",
    "        \"PAD\",\n",
    "        \"PAD\",\n",
    "        \"PAD\",\n",
    "        \"PAD\"\n",
    "    ]\n",
    "}\n",
    "\n",
    "#we no longer use neighbors as it didnt work well\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "os.environ[\"SUMO_HOME\"] = \"C:/Users/alien/AppData/Local/Programs/Python/Python39/Lib/site-packages/sumo\"\n",
    "import pickle\n",
    "\n",
    "env = SumoEnvironment(\n",
    "    net_file=\"buffalo/buffalo_final.net.xml\",\n",
    "    route_file=\"buffalo/phase2.rou.xml\",\n",
    "    use_gui=True,    \n",
    "    num_seconds=2500, \n",
    "    single_agent=False,\n",
    "\n",
    "    delta_time=5,\n",
    "    reward_fn='pressure'\n",
    ")\n",
    "\n",
    "def winner_winner_chicken_dinner(self, obs):\n",
    "    phase = int(obs[0])\n",
    "    queues = obs[1:]\n",
    "\n",
    "    binned = []\n",
    "    for q in queues:\n",
    "        if q < 0.33:\n",
    "            #print(q)\n",
    "            binned.append(0)\n",
    "        elif q < 0.66:\n",
    "            binned.append(1)\n",
    "        else:\n",
    "            binned.append(2)\n",
    "\n",
    "    return (phase, *binned)\n",
    "saved_weights =[]\n",
    "with open(\"ergregoi_dviola_ub_medical_campus_ddqn_weights\", \"rb\") as f:\n",
    "    saved_weights = pickle.load(f)\n",
    "\n",
    "agents = {}\n",
    "buffers = {}\n",
    "for traffic_id in traffic_ids:\n",
    "\n",
    "    agent = DQNAgent(\n",
    "        action_space=env.traffic_signals[traffic_id].num_green_phases,\n",
    "        initial_epsilon= 0,\n",
    "        min_epsilon=0,\n",
    "        epsilon_decay=1,\n",
    "        lr=0.00005,\n",
    "        gamma=.95\n",
    "\n",
    "    )\n",
    "\n",
    "\n",
    "    if traffic_id in saved_weights:\n",
    "        agent.load_state_dict(saved_weights[traffic_id])\n",
    "\n",
    "        agent.update_target_network()\n",
    "\n",
    "    print(env.traffic_signals[traffic_id].num_green_phases, traffic_id)\n",
    "    agents[traffic_id] = agent\n",
    "\n",
    "\n",
    "    buffers[traffic_id] = ReplayBuffer(max_size=10000)\n",
    "\n",
    "\n",
    "num_episodes  = 3\n",
    "batch_size= 86\n",
    "target_update_freq = 4\n",
    "eval_freq=5500\n",
    "best_reward=-100000000000\n",
    "phase_locker ={traffic_id: 0 for traffic_id in traffic_ids }\n",
    "\n",
    "\n",
    "evaluation=True\n",
    "prev_actions = {traffic_id: 0 for traffic_id in traffic_ids}\n",
    "if evaluation ==False:\n",
    "    for episode in range(num_episodes):\n",
    "        timestep_counter = 0\n",
    "        print(episode)\n",
    "\n",
    "        obs = env.reset()\n",
    "        done = False\n",
    "        total_reward = {tid: 0 for tid in traffic_ids}\n",
    "\n",
    "        phase = {tid: obs[tid][0] for tid in traffic_ids}\n",
    "        queue = {tid: obs[tid][1] for tid in traffic_ids}\n",
    "\n",
    "        while not done:\n",
    "            timestep_counter += 1\n",
    "\n",
    "            if timestep_counter % 400 == 0:\n",
    "                for traffic_id in traffic_ids:\n",
    "                    agent = agents[traffic_id]\n",
    "                    agent.epsilon = max(agent.min_epsilon, agent.epsilon * agent.epsilon_decay)\n",
    "                    #print(agent.epsilon)\n",
    "            actions = {}\n",
    "            for traffic_id in traffic_ids:\n",
    "                state = [phase[traffic_id], queue[traffic_id]]\n",
    "                discretized_state = winner_winner_chicken_dinner(None, state)\n",
    "\n",
    "                if phase_locker[traffic_id] <= 0:\n",
    "                    action = agents[traffic_id].step(discretized_state)\n",
    "                    prev_actions[traffic_id] = action\n",
    "                    phase_locker[traffic_id] = 3\n",
    "                else:\n",
    "                    action = prev_actions[traffic_id]\n",
    "                    phase_locker[traffic_id] -= 1\n",
    "\n",
    "                valid_actions = list(range(env.traffic_signals[traffic_id].num_green_phases))\n",
    "                if action not in valid_actions:\n",
    "                    action = random.choice(valid_actions)\n",
    "\n",
    "                actions[traffic_id] = action\n",
    "\n",
    "            next_obs, reward_, terminated, info = env.step(actions)\n",
    "\n",
    "            phase_next ={tid: next_obs[tid][0 ] for tid in traffic_ids}\n",
    "            queue_next = {tid: next_obs[tid][1] for tid in traffic_ids}\n",
    "\n",
    "            for tid in traffic_ids:\n",
    "                curr_state= [phase[tid], queue[tid]]\n",
    "                next_state = [phase_next[tid], queue_next[tid]]\n",
    "\n",
    "                disc_curr= winner_winner_chicken_dinner(None,curr_state)\n",
    "                disc_next= winner_winner_chicken_dinner(None, next_state)\n",
    "\n",
    "                buffers[tid].update_buffer(disc_curr, actions[tid],  reward_[tid], disc_next, terminated[tid])\n",
    "                total_reward[tid] += reward_[tid]\n",
    "\n",
    "                if len(buffers[tid].buffer) >= batch_size and episode % 4 == 0:\n",
    "                    batch = buffers[tid].sample_batch(batch_size)\n",
    "                    agents[tid].training_step(batch)\n",
    "\n",
    "            obs = next_obs\n",
    "            phase = phase_next\n",
    "            queue = queue_next\n",
    "            done = terminated.get(\"__all__\", False)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "        if (episode+1)%target_update_freq==0:\n",
    "            for t in traffic_ids:\n",
    "                agents[t].update_target_network()\n",
    "        w={}\n",
    "        w2={}\n",
    "\n",
    "        avg_r=sum(total_reward.values())/len(total_reward)\n",
    "        if avg_r > best_reward:\n",
    "\n",
    "            best_reward=avg_r\n",
    "            for t in traffic_ids:\n",
    "                w[t]=agents[t].state_dict()\n",
    "            with open(\"dqn_traffic_agents_best.pkl3\",\"wb\") as ff:\n",
    "                pickle.dump(w,ff)\n",
    "\n",
    "        for t in traffic_ids:\n",
    "            w2[t]=agents[t].state_dict()\n",
    "        with open(\"dqn_traffic_agents.pkl3\",\"wb\") as ff:\n",
    "            pickle.dump(w,ff)\n",
    "\n",
    "if evaluation == True:\n",
    "    prev_actions = {traffic_id: 0 for traffic_id in traffic_ids}\n",
    "\n",
    "    for episode in range(num_episodes):\n",
    "        timestep_counter = 0\n",
    "        print(episode)\n",
    "        obs = env.reset()\n",
    "        done = False\n",
    "        total_reward = {tid: 0 for tid in traffic_ids}\n",
    "\n",
    "        phase = {tid: obs[tid][0] for tid in traffic_ids}\n",
    "        queue = {tid: obs[tid][1] for tid in traffic_ids}\n",
    "\n",
    "        while not done:\n",
    "            timestep_counter += 1\n",
    "\n",
    "            actions = {}\n",
    "            for traffic_id in traffic_ids:\n",
    "                state = [phase[traffic_id], queue[traffic_id]]\n",
    "                discretized_state = winner_winner_chicken_dinner(None, state)\n",
    "\n",
    "                if phase_locker[traffic_id] <= 0:\n",
    "                    action = agents[traffic_id].step(discretized_state)\n",
    "                    prev_actions[traffic_id] = action\n",
    "                    phase_locker[traffic_id] = 3\n",
    "                else:\n",
    "                    action = prev_actions[traffic_id]\n",
    "                    phase_locker[traffic_id] -= 1\n",
    "\n",
    "                valid_actions = list(range(env.traffic_signals[traffic_id].num_green_phases))\n",
    "                if action not in valid_actions:\n",
    "                    action = random.choice(valid_actions)\n",
    "\n",
    "                actions[traffic_id] = action\n",
    "\n",
    "            next_obs, reward_, terminated, info = env.step(actions)\n",
    "\n",
    "            phase_next = {tid: next_obs[tid][0] for tid in traffic_ids}\n",
    "            queue_next = {tid: next_obs[tid][1] for tid in traffic_ids}\n",
    "\n",
    "            for tid in traffic_ids:\n",
    "                total_reward[tid] += reward_[tid]\n",
    "\n",
    "            obs = next_obs\n",
    "            phase =phase_next\n",
    "            queue =queue_next\n",
    "            done = terminated.get(\"__all__\", False)\n",
    "\n",
    "        avg_r = sum(total_reward.values()) / len(total_reward)\n",
    "        print(avg_r)\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
